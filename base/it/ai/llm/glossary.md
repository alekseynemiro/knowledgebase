---
title: "LLM: Глоссарий"
description: Глоссарий терминов, связанных с LLM.
tags:
  - Glossary
  - LLM
  - Large Language Models
  - Machine Learning
  - AI
  - Глоссарий
  - Большие языковые модели
  - Машинное обучение
  - ИИ
---

# LLM: Глоссарий

* **ALBERT** (A Lite BERT) — уменьшенная версия BERT, оптимизированная для более эффективного обучения с сохранением производительности.
* **Accuracy** — метрика, измеряющая частоту правильного прогнозирования результата моделью. Она рассчитывается как отношение количества правильно предсказанных случаев к общему количеству случаев в наборе данных.
* **Action** (действие) — решение агента, основанное на текущем состоянии окружения (environment).
* **Agent** (агент) — это сущность, которая взаимодействует с большой языковой моделью и оценивает корректность полученного ответа. При необходимости корректирует модель для получения желаемого результата.
* **Annotated Dataset** — это тип текстового набора данных, который содержит специальную разметку, позволяющую моделям понимать, как интерпретировать данные.
* **BERT** (Bidirectional Encoder Representations from Transformers) — тип большой языковой модели на архитектуре трансформатора (transformer), в основе которой лежит принцип предварительного обучения модели на больших текстовых корпусах для последующей тонкой настройки под конкретные задачи.
* **Beginning Of Sentence** (BOS) — это специальный токен, обозначающий начало предложения. Для моделей семейства Llama — `<s>`.
* **Code Dataset** — это тип набора данных, содержащий примеры кода и комментарии к ним.
* **Corpus** (корпус) — это большой и структурированный набор текстов, используемый для обучения больших языковых моделей.
* **Dataset** (набор данных) — совокупность упорядоченных данных. Наборы данных могут быть представлены в различных форматах, таких как таблицы (например, CSV-файлы), базы данных или специализированные форматы данных.
* **Dialogue Dataset** — это тип набора данных, содержащий беседы между двумя или более участниками.
* **ELECTRA** — тип большой языковой модели, в которой вместо маскировки токенов (слов) используется дискриминационный подход к обучению.
* **Embedding** (внедрение, встраивание) — внедрение в модель данных.
* **End Of Sentence** (EOS) — это специальный токен, обозначающий конец предложения. Для моделей семейства Llama — `</s>`.
* **Environment** (окружение, окружающая среда) — внешний мир, с которым взаимодействует агент (agent).
* **F1 Score** — метрика, используемая для оценки эффективности модели классификации, особенно для несбалансированных наборов данных; это среднее гармоническое значение точности и полноты.
* **FAISS** (Facebook AI Similarity Search) — легковесная векторная база данных, разработанная Facebook.
* **Fine-Tuning** — процесс адаптации предварительно обученной языковой модели. См. также IFT.
* **GGUF** (GGML Universal File) — двоичный формат файла для хранения тензоров и метаданных модели для llama.cpp.
* **GPT** (Generative Pre-trained Transformer) — тип большой языковой модели, основанной на архитектуре глубокого обучения Transformer, предварительно обученной на больших наборах данных и способной генерировать человекоподобный контент.
* **Goal** — долгосрочная цель, которую агент (agent) стремится достичь.
* **HNSW** (Hierarchical Navigable Small World) — структура данных для эффективного выполнения приблизительного поиска ближайшего соседа.
* **IFT** (Instruction Fine-Tuning) — процесс адаптации предварительно обученной языковой модели для выполнения определенных инструкций или задач, что позволяет улучшить её производительность при выполнении этих задач.
* **IVF** (Inverted Index, or Inverted File) — структура данных, используемая при поиске информации, которая позволяет эффективно искать документы, содержащее определенные слова.
* **IVFPQ** (IVF with Product Quantization) — представляет собой составной индекс, который объединяет инвертированный файловый индекс (IVF) и квантование продукта (PQ) для сокращения объема хранилища и повышения эффективности.
* **Inference** — процесс получения прогнозов с помощью обученной модели. Проще говоря, это генерация текста моделью в режиме реального времени.
* **Instructional Dataset** (instruct) — это тип набора данных, содержащий команды или инструкции для модели.
* **LLM** (Large Language Model) — тип нейронной сети, предназначенный для понимания, генерации и обработки естественного языка (natural language) путём прогнозирования вероятности последовательностей слов.
* **LangChain** — набор решений на Python для интеграции больших языковых моделей (LLM). Также существует реализация-обёртка для Node.js.
* **LangGraph** — набор решений на Python для разработки рабочих процессов с использованием LLM. Рабочие процессы реализуются в виде графов. Является продолжением развития библиотеки LangChain. Существует обёртка на Node.js.
* **Llama** (Large Language Model Meta AI, aka LLaMA) — это семейство авторегрессионных больших языковых моделей (LLM), выпускаемых компанией Meta AI с февраля 2023 года.
* **Llama.cpp** — программное обеспечения с открытым исходным кодом для работы с большими языковыми моделями, такими как Llama.
* **Min P** — метод выборки рассматривает возможность включения слов с вероятностью не менее `P`. Используется как дополнение к **Top K** и **Top P**.
* **NLP** (Natural Language Processing) — область искусственного интеллекта, сосредоточенная на взаимодействии между компьютерами и людьми посредством естественного языка, что позволяет машинам понимать, интерпретировать и генерировать человеческие языки.
* **PEFT** (Parameter-Efficient Fine-Tuning) — это метод адаптации предварительно обученных моделей с минимальными изменениями параметров модели, что делает тонкую настройку более эффективной и менее ресурсоёмкой.
* **Padding** (PAD) — специальный токен для унификации длины последовательности в пакете. Большинство моделей требуют, чтобы размер текста в последовательности были одинакового размера, этот токен помогает решить эту проблему.
* **Parallel Corpora** — это тип набора данных, содержащий записи на разных языках. Используется для обучения моделей машинного перевода.
* **Parameters** (параметры) — переменные нейронной сети, которые задают поведение модели в процессе обучения.
* **Perplexity** — единица измерения, которая показывает насколько хорошо языковая модель предсказывает. Чем ниже значение perplexity, тем лучше модель отражает заданный текст.
* **Policy** — это стратегия агента, которая определяет, какие действия он выбирает в зависимости от состояния окружения (environment).
* **Precision** — метрика, которая оценивает, какая доля всех объектов, классифицированных алгоритмом как положительные, на самом деле являются положительными.
* **Prompt Engineering** — это процесс структурирования или создания инструкций с целью получения наилучшего результата от больших языковых моделей.
* **Prompt Tuning** — подход к настройке поведения модели путем изменения текста подсказки.
* **Prompt** — представляет входные данные - инструкцию, которая задаёт контекст и поведение модели.
* **Quantization** — в языковых моделях это процесс ограничения округления чисел для уменьшения объёма потребляемой оперативной памяти. Чем меньше значение, тем быстрее работа, но тем менее точный результат.
* **RAG** (Relative-Augmented Generation) — это метод, который предоставляет генеративным моделям искусственного интеллекта возможности поиска информации путем интеграции внешних систем поиска, улучшения их контекста и релевантности.
* **Recall** — метрика, которая измеряет долю фактических положительных случаев, которые были правильно идентифицированы моделью.
* **Reinforcement Learning Environment** (RL, Reinforcement Learning) — область машинного обучения, в которой агент учится взаимодействовать с окружающей средой (environment) для достижения определенной цели на основе обратной связи в виде вознаграждения.
* **Reward** — обратная связь, которую агент получает после выполнения действия. Это может быть как положительное (для правильных действий), так и отрицательное (для неправильных действий) значение.
* **RoBERTa** (A Robustly Optimized BERT Pre-training Approach) — тип большой языковой модели, улучшенная версия BERT, с более строгими и оптимизированными условиями обучения.
* **RoPE** (Rotary Position Embedding) — метод добавления позиционной информации к последовательностям данных, используемый в моделях трансформаторов.
* **SFT** (Supervised Fine-Tuning) — процесс адаптации предварительно обученной модели на основе маркированных данных с целью повышения её производительности для решения конкретных задач путем её точной настройки с помощью методов контролируемого обучения.
* **Sampling** — используется для уменьшения объемов данных без потери информации.
* **Separator** (SEP) - специальный токен для разделения фрагментов текста внутри одного примера. В какой-то степени, это можно назвать параграфом или абзацем.
* **Supervised Learning Dataset** — учебный набор данных.
* **T5** (Text-to-Text Transfer Transformer) — семейство больших языковых моделей от Google для преобразования текста в текст. Например, модель можно использовать для задач суммаризации, классификации, перевода с одного языка на другой и т.п.
* **Temperature** — параметр, используемый при генерации текста для управления степенью разнообразия. Он корректирует распределение вероятностей возможных следующих слов: более низкие температуры делают модель более детерминированной и консервативной, в то время как более высокие температуры стимулируют разнообразие и креативность в результатах, делая менее вероятные прогнозы более вероятными. Не рекомендуется использовать значения `0.0` и `1.0`, поскольку результат может быть неожиданным.
* **Text corpora** — большая коллекция текстовых данных, объединенных вместе, которая обычно используется для обучения моделей на ранних этапах их жизни.
* **Token** (code, маркер, токен) — базовая единица ввода и вывода в языковой модели. Токен обычно представляет слово, часть слова или символ.
* **Top K** — метод выборки ограничивает выборку прогнозируемых слов только наиболее вероятными вариантами из распределения. Чем меньше значение, тем хуже результат.
* **Top P** (nucleus sampling) — метод выборки включает все слова с кумулятивной вероятностью, большей или равной заданному порогу `P`. Чем ниже значение, тем хуже результат.
* **Transfer Learning** — обучение модели на основе предыдущего опыта.
* **Transformer** — архитектура глубокого обучения, разработанная Google и основанная на механизме многоголового (multi-headed) внимания. Текст преобразуется в числовые представления, называемые токенами, а каждый токен преобразуется в вектор посредством поиска по таблице векторных представлений слов.
* **Unknown** (UNK) — специальный токен, представляющий неизвестное или редко встречающееся слово. На практике, используется редко. Для моделей семейства Llama - `<unk>`.
* **Vectorization** (векторизация) — процесс преобразования текста в числовые векторы, которые затем можно индексировать и использовать для быстрого поиска.
* **Weight** (вес) — числовые коэффициенты, описывающие работу каждого нейрона в сети.
